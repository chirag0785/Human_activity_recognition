# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C7sc6N0peI271sG0QiuNMBkvGauNnbEX
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
# # Read the features.txt file
# features = pd.read_table('/content/features.txt', sep=' ', header=None, names=('ID', 'Feature'))

# # Read the X_train.txt file
# X_train = pd.read_table('/content/X_train.txt', sep='\s+', header=None)
# X_train.columns = features['Feature']


# X_test = pd.read_table('/content/X_test.txt', sep='\s+', header=None)
# X_test.columns = features['Feature']


# # Read the y_train.txt file
# y_train = pd.read_table('/content/y_train.txt', sep=' ', header=None)
# y_train.columns = ['Activity']

# y_test=pd.read_table('/content/y_test.txt', sep=' ', header=None)
# # Combine all the data into a single dataframe
# all_data = pd.concat([y_train, X_train], axis=1)

# all_data2=pd.concat([X_test,y_test],axis=1);
# # Save the data to a CSV file
# all_data.to_csv("train.csv", index=False)
# all_data2.to_csv("test.csv",index=False)



df=pd.read_csv('/content/train.csv');
testdf=pd.read_csv('/content/test.csv');

df.head()

df.tail()

#checking for null values
df.isnull().sum()

#checking for duplicate rows
df.duplicated().any()

df.describe()

#Shape of the dataset
df.shape
df.info()

#duplicated columns are stored in duplicated_columns
duplicated_columns=df.columns[df.T.duplicated()].tolist()

duplicated_columns

len(duplicated_columns)

#dropping the duplicate columns and dataframe after droppping the duplicate columns is stored in df
df=df.drop(duplicated_columns,axis=1);
testdf=testdf.drop(duplicated_columns,axis=1);

# columns reduced to 541
df.shape

#plotting the target variable
sns.countplot(data=df, x='Activity')
plt.xlabel('Activity')
plt.ylabel('Count')
plt.title('Distribution of Activity')
plt.xticks(rotation=45)

# Separate features and labels
X = df.drop(columns=['Activity'])
y = df['Activity']

from imblearn.under_sampling import RandomUnderSampler
from collections import Counter
# Instantiate RandomUnderSampler
rus = RandomUnderSampler(random_state=42)

# Perform undersampling
X_resampled, y_resampled = rus.fit_resample(X, y)

# Print class distribution before and after undersampling
print("Class distribution before undersampling:", Counter(y))
print("Class distribution after undersampling:", Counter(y_resampled))

# If you still want to convert the resampled data back to DataFrame, you can do it as follows:
df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Activity')], axis=1)

sns.countplot(data=df, x='Activity')
plt.xlabel('Activity')
plt.ylabel('Count')
plt.title('Distribution of Activity')
plt.xticks(rotation=45)
plt.show()

#store feature matrix in X and response vector in Y
X = df.drop('Activity', axis = 1)
Y = df['Activity']
X_test = testdf.drop('0', axis=1)

# Target variable for testing (y_test)
y_test = testdf['0']

# VARIANCE THRESHOLD In this section i will drop the constant features
from sklearn.feature_selection import VarianceThreshold
var_thres=VarianceThreshold(threshold =0)
var_thres.fit(X)

#Finding non constant features
sum(var_thres.get_support())

var_thres.get_support()

# import seaborn as sns
# import matplotlib.pyplot as plt

# # Assuming you have a DataFrame df and computed the correlation matrix
# correlation_matrix = df.corr()
# # Create a heatmap
# plt.figure(figsize=(8, 6))
# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
# plt.title("Correlation Matrix Heatmap")
# plt.show()

#Feature correlation
X.corr()

#using pearson correlation
# plt.figure(figsize = (12,10))
# cor= X.corr()
# sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)
# plt.show()

# with the following function we can select highly correlated features
# it will remove the first feature that is correlated with anything other feature

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X, 0.7)
len(set(corr_features))

corr_features

X = X.drop(corr_features,axis=1)
X_test=X_test.drop(corr_features,axis=1);

print(X.shape)
print(X_test.shape)

from sklearn.feature_selection import mutual_info_classif
# determine the mutual information
mutual_info = mutual_info_classif(X,Y)
mutual_info

mutual_info = pd.Series(mutual_info)
mutual_info.index = X.columns
mutual_info.sort_values(ascending=False)

#let's plot the ordered mutual_info values per feature
mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))

from sklearn.feature_selection import SelectKBest
print(X.shape,Y.shape)

#No we Will select the  top 70 important features
selector = SelectKBest(score_func=mutual_info_classif, k=70)
X_train = selector.fit_transform(X, Y)
X_test = selector.transform(X_test)



all_columns_values = X_train[:, :]
print(all_columns_values)



all_columns_values = X_test[:, :]
print(all_columns_values)

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from collections import Counter
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report
import seaborn as sns
import matplotlib.pyplot as plt

class RandomForest:
    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, n_features=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.n_features = n_features
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            tree = DecisionTreeClassifier(max_depth=self.max_depth,
                                          min_samples_split=self.min_samples_split,
                                          max_features=self.n_features)
            X_sample, y_sample = self._bootstrap_samples(X, y)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)

    def _bootstrap_samples(self, X, y):
        n_samples = X.shape[0]
        idxs = np.random.choice(n_samples, n_samples, replace=True)
        return X.iloc[idxs], y.iloc[idxs]

    def predict(self, X):
        predictions = np.array([tree.predict(X) for tree in self.trees])
        predictions = np.swapaxes(predictions, 0, 1)
        predictions = np.array([self._most_common_label(pred) for pred in predictions])
        return predictions

    def _most_common_label(self, y):
        counter = Counter(y)
        value = counter.most_common(1)[0][0]
        return value
X_train2 = pd.DataFrame(X_train)
Y2 = pd.Series(Y)

clf = RandomForest()
clf.fit(X_train2, Y2)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Calculate precision
precision = precision_score(y_test, y_pred,average='weighted')

# Calculate recall
recall = recall_score(y_test, y_pred,average='weighted')

# Calculate F1-score
f1 = f1_score(y_test, y_pred,average='weighted')

# Print precision, recall, and F1-score
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Generate classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

cm = confusion_matrix(y_test, y_pred)

tn = cm[0, 0]
fp = cm[0, 1]
fn = cm[1, 0]
tp = cm[1, 1]
print(cm);

print(X_train[0:5,:]);
print(Y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# Initialize Random Forest classifier
clf = RandomForestClassifier(n_estimators=100)

# Train the classifier
clf.fit(X_train2, Y2)

# Compute predicted probabilities for each class
y_scores = clf.predict_proba(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(len(np.unique(Y2))):
    fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_scores[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves for each class
plt.figure(figsize=(8, 6))
for i in range(len(np.unique(Y2))):
    plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve of class {0} (AUC = {1:0.2f})'.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC Curve')
plt.legend(loc="lower right")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
import numpy as np

# Initialize Random Forest classifier
clf = RandomForestClassifier(n_estimators=100)

# Train the classifier
clf.fit(X_train2, Y2)

# Compute predicted probabilities for each class
y_scores = clf.predict_proba(X_test)

# Compute precision-recall curve and precision-recall area for each class
precision = dict()
recall = dict()
average_precision = dict()
for i in range(len(np.unique(Y2))):
    precision[i], recall[i], _ = precision_recall_curve((y_test == i).astype(int), y_scores[:, i])
    average_precision[i] = np.mean(precision[i])

# Plot precision-recall curves for each class
plt.figure(figsize=(8, 6))
for i in range(len(np.unique(Y2))):
    plt.plot(recall[i], precision[i], lw=2, label='Precision-recall curve of class {0} (AP = {1:0.2f})'.format(i, average_precision[i]))

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# Initialize Random Forest classifier
clf = RandomForestClassifier();

# Compute cross-validation scores for accuracy
cv_scores_accuracy = cross_val_score(clf, X_train, Y, cv=5, scoring='accuracy')

# Compute cross-validation scores for other metrics like precision, recall, F1-score
# Example:
# cv_scores_precision = cross_val_score(clf, X_train, y_train, cv=5, scoring='precision_macro')
# cv_scores_recall = cross_val_score(clf, X_train, y_train, cv=5, scoring='recall_macro')
# cv_scores_f1 = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1_macro')

# Print mean and standard deviation of cross-validation scores
print("Cross-validation scores for accuracy:", cv_scores_accuracy)
print("Mean accuracy:", np.mean(cv_scores_accuracy))
print("Standard deviation of accuracy:", np.std(cv_scores_accuracy))

# Repeat the above steps for other metrics if needed

# class SVM:

#     def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):
#         self.lr = learning_rate
#         self.lambda_param = lambda_param
#         self.n_iters = n_iters
#         self.w = None
#         self.b = None

#     def fit(self, X, y):
#         n_samples, n_features = X.shape

#         y_ = np.where(y <= 0, -1, 1)

#         # init weights
#         self.w = np.zeros(n_features)
#         self.b = 0

#         for _ in range(self.n_iters):
#             for idx, x_i in enumerate(X):
#                 condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1
#                 if condition:
#                     self.w -= self.lr * (2 * self.lambda_param * self.w)
#                 else:
#                     self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))
#                     self.b -= self.lr * y_[idx]


#     def predict(self, X):
#         approx = np.dot(X, self.w) - self.b
#         return np.sign(approx)

# clf = SVM()
# clf.fit(X_train, Y)
# predictions = clf.predict(X_test)

# def accuracy(y_true, y_pred):
#     accuracy = np.sum(y_true == y_pred) / len(y_true)
#     return accuracy


# print("SVM classification accuracy", accuracy(y_test, predictions))

def euclidean_distance(x1, x2):
    distance = np.sqrt(np.sum((x1-x2)**2))
    return distance

class KNN:
    def __init__(self, k=3):
        self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        predictions = [self._predict(x) for x in X]
        return predictions

    def _predict(self, x):
        # compute the distance
        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]

        # get the closest k
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]

        # majority voye
        most_common = Counter(k_nearest_labels).most_common()
        return most_common[0][0]
clf = KNN(k=5)
clf.fit(X_train, Y)
y_pred = clf.predict(X_test)

acc = np.sum(y_pred == y_test) / len(y_test)
print(acc)
precision = precision_score(y_test, y_pred,average='weighted')

# Calculate recall
recall = recall_score(y_test, y_pred,average='weighted')

# Calculate F1-score
f1 = f1_score(y_test, y_pred,average='weighted')

# Print precision, recall, and F1-score
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Generate classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

cm = confusion_matrix(y_test, y_pred)

tn = cm[0, 0]
fp = cm[0, 1]
fn = cm[1, 0]
tp = cm[1, 1]
print(cm)

# def sigmoid(x):
#     return 1/(1+np.exp(-x))

# class LogisticRegression():

#     def __init__(self, lr=0.001, n_iters=1000):
#         self.lr = lr
#         self.n_iters = n_iters
#         self.weights = None
#         self.bias = None

#     def fit(self, X, y):
#         n_samples, n_features = X.shape
#         self.weights = np.zeros(n_features)
#         self.bias = 0

#         for _ in range(self.n_iters):
#             linear_pred = np.dot(X, self.weights) + self.bias
#             predictions = sigmoid(linear_pred)

#             dw = (1/n_samples) * np.dot(X.T, (predictions - y))
#             db = (1/n_samples) * np.sum(predictions-y)

#             self.weights = self.weights - self.lr*dw
#             self.bias = self.bias - self.lr*db


#     def predict(self, X):
#         linear_pred = np.dot(X, self.weights) + self.bias
#         y_pred = sigmoid(linear_pred)
#         class_pred = [0 if y<=0.5 else 1 for y in y_pred]
#         return class_pred
# clf = LogisticRegression(lr=0.01)
# clf.fit(X_train,Y)
# y_pred = clf.predict(X_test)

# acc = accuracy(y_pred, y_test)
# print(acc)
# precision = precision_score(y_test, y_pred,average='weighted')

# # Calculate recall
# recall = recall_score(y_test, y_pred,average='weighted')

# # Calculate F1-score
# f1 = f1_score(y_test, y_pred,average='weighted')

# # Print precision, recall, and F1-score
# print("Precision:", precision)
# print("Recall:", recall)
# print("F1-score:", f1)

# # Generate classification report
# report = classification_report(y_test, y_pred)
# print("Classification Report:\n", report)

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import export_graphviz
import graphviz

# Assuming you have already split your data into X_train, y_train, X_test, and y_test

# Train a model
model = RandomForestClassifier()
model.fit(X_train, Y)

# Make predictions
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

tn = cm[0, 0]
fp = cm[0, 1]
fn = cm[1, 0]
tp = cm[1, 1]

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Calculate precision
precision = precision_score(y_test, y_pred,average='weighted')

# Calculate recall
recall = recall_score(y_test, y_pred,average='weighted')

# Calculate F1-score
f1 = f1_score(y_test, y_pred,average='weighted')

# Print precision, recall, and F1-score
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Generate classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

print("Confusion Matrix")
print(cm)

model = DecisionTreeClassifier()
model.fit(X_train, Y)

# Make predictions
y_pred = model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)

tn = cm[0, 0]
fp = cm[0, 1]
fn = cm[1, 0]
tp = cm[1, 1]

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Calculate precision
precision = precision_score(y_test, y_pred,average='weighted')

# Calculate recall
recall = recall_score(y_test, y_pred,average='weighted')

# Calculate F1-score
f1 = f1_score(y_test, y_pred,average='weighted')

# Print precision, recall, and F1-score
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Generate classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

print("Confusion Matrix")
print(cm)

from sklearn.neighbors import KNeighborsClassifier
k = 3
knn_model = KNeighborsClassifier(n_neighbors=k)

# Train the KNN model on the training data
knn_model.fit(X_train, Y)

# Make predictions on the testing data
y_pred = knn_model.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

tn = cm[0, 0]
fp = cm[0, 1]
fn = cm[1, 0]
tp = cm[1, 1]

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Calculate precision
precision = precision_score(y_test, y_pred,average='weighted')

# Calculate recall
recall = recall_score(y_test, y_pred,average='weighted')

# Calculate F1-score
f1 = f1_score(y_test, y_pred,average='weighted')

# Print precision, recall, and F1-score
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Generate classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

print("Confusion Matrix")
print(cm)

from sklearn.svm import SVC
svm_model = SVC(kernel='linear', C=1.0)

# Train the SVM model on the training data
svm_model.fit(X_train, Y)

# Make predictions on the testing data
y_pred = svm_model.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

tn = cm[0, 0]
fp = cm[0, 1]
fn = cm[1, 0]
tp = cm[1, 1]

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


from sklearn.metrics import precision_score, recall_score, f1_score, classification_report

# Calculate precision
precision = precision_score(y_test, y_pred,average='weighted')

# Calculate recall
recall = recall_score(y_test, y_pred,average='weighted')

# Calculate F1-score
f1 = f1_score(y_test, y_pred,average='weighted')

# Print precision, recall, and F1-score
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# Generate classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

print("Confusion Matrix")
print(cm)

